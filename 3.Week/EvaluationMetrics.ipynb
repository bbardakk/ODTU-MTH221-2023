{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf008fbb-a44a-4ad2-95e0-f2c3b3960dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da7f1c6c-a455-4d45-8b46-7431832e9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example predictions\n",
    "y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
    "y_pred = [0, 1, 1, 1, 1, 0, 0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "910ac09d-c816-416e-b7d5-aefa63b5a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "192a2ff0-94b4-4cf6-bfcb-143685d7324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [1 4]]\n",
      "\n",
      "True Positives (TP): 4\n",
      "True Negatives (TN): 3\n",
      "False Positives (FP): 2\n",
      "False Negatives (FN): 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nTrue Positives (TP):\", tp)\n",
    "print(\"True Negatives (TN):\", tn)\n",
    "print(\"False Positives (FP):\", fp)\n",
    "print(\"False Negatives (FN):\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a17a2a0f-28d4-441d-b3c5-1ea62d9ba486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.7\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.8\n",
      "F1 Score: 0.7272727272727272\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e23a07-8a5f-4246-a2d0-61089728969c",
   "metadata": {},
   "source": [
    "2. Why Accuracy Can Be Misleading:\n",
    "\n",
    "Imagine a scenario where only 1% of a large dataset belongs to the positive class (e.g., detecting a rare disease). Even if a model predicts the negative class for all instances, its accuracy would still be 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3285070a-659d-42f3-9ed3-e5b33120d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated rare disease dataset\n",
    "\n",
    "y_true_rare = [1] + [0] * 99\n",
    "y_pred_rare = [0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc4cf2e7-ec6c-4212-b927-2805495d011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for rare disease dataset: 0.99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "accuracy_rare = accuracy_score(y_true_rare, y_pred_rare)\n",
    "print(\"\\nAccuracy for rare disease dataset:\", accuracy_rare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dab2b-cc62-42a1-aba4-0e649b583e6e",
   "metadata": {},
   "source": [
    "3. Different Costs for FP and FN:\n",
    " \n",
    "In some scenarios, false positives and false negatives have different implications. For instance, consider spam email detection:\n",
    "\n",
    "False Positive (FP): Marking a legitimate email as spam.\n",
    "False Negative (FN): Failing to mark a spam email as spam.\n",
    "Here, FPs might be costlier because important emails could be missed, while FNs just mean some spam gets through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4555b3bc-5206-4123-8565-c5316d4acebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total cost for email predictions: 7\n"
     ]
    }
   ],
   "source": [
    "# Simulated email predictions\n",
    "y_true_email = [1, 0, 1, 1, 0, 0, 1, 0, 0, 0]  # 1 is spam, 0 is legitimate\n",
    "y_pred_email = [0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
    "\n",
    "cm_email = confusion_matrix(y_true_email, y_pred_email)\n",
    "tn_email, fp_email, fn_email, tp_email = cm_email.ravel()\n",
    "\n",
    "cost_fp = 5  # cost of marking a legitimate email as spam\n",
    "cost_fn = 2   # cost of failing to mark a spam email\n",
    "\n",
    "total_cost = fp_email * cost_fp + fn_email * cost_fn\n",
    "print(\"\\nTotal cost for email predictions:\", total_cost)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
